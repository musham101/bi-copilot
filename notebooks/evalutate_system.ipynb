{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e35a4588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import math\n",
    "import statistics\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "add8e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"../data\"\n",
    "REPORTS_DIR = \"../results\"\n",
    "\n",
    "API_URL = \"http://localhost:8000/query\"\n",
    "API_TIMEOUT_S = 120\n",
    "\n",
    "USE_OPENAI_JUDGE = True\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "OPENAI_TIMEOUT_S = 60\n",
    "JUDGE_SAMPLE_LIMIT = None     # set int (e.g., 200) to judge only first N rows\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb917d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_KEYWORDS = {\n",
    "    \"select\",\"from\",\"where\",\"join\",\"inner\",\"left\",\"right\",\"full\",\"cross\",\"on\",\n",
    "    \"group\",\"by\",\"having\",\"order\",\"limit\",\"offset\",\"distinct\",\"union\",\"all\",\n",
    "    \"insert\",\"into\",\"values\",\"update\",\"set\",\"delete\",\"case\",\"when\",\"then\",\"else\",\"end\",\n",
    "    \"as\",\"and\",\"or\",\"not\",\"in\",\"exists\",\"between\",\"like\",\"is\",\"null\"\n",
    "}\n",
    "\n",
    "def normalize_sql(sql: str) -> str:\n",
    "    \"\"\"Normalization that preserves structure while removing formatting noise.\"\"\"\n",
    "    if sql is None:\n",
    "        return \"\"\n",
    "    s = sql.strip()\n",
    "    s = s.rstrip(\";\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)          # collapse whitespace\n",
    "    s = s.replace(\"`\", \"\")              # remove backticks (optional)\n",
    "    return s.lower().strip()\n",
    "\n",
    "def sql_tokens(sql: str) -> List[str]:\n",
    "    \"\"\"Simple SQL tokenization (no external deps).\"\"\"\n",
    "    s = normalize_sql(sql)\n",
    "    toks = re.findall(r\"[a-z_]+|\\d+|<=|>=|!=|=|<|>|\\*|\\(|\\)|,|\\.\", s)\n",
    "    return [t for t in toks if t.strip()]\n",
    "\n",
    "def token_f1(pred: str, gold: str) -> Tuple[float, float, float]:\n",
    "    p = sql_tokens(pred)\n",
    "    g = sql_tokens(gold)\n",
    "    if not p and not g:\n",
    "        return (1.0, 1.0, 1.0)\n",
    "    if not p or not g:\n",
    "        return (0.0, 0.0, 0.0)\n",
    "\n",
    "    from collections import Counter\n",
    "    pc = Counter(p)\n",
    "    gc = Counter(g)\n",
    "\n",
    "    common = 0\n",
    "    for k in pc.keys():\n",
    "        common += min(pc[k], gc.get(k, 0))\n",
    "\n",
    "    precision = common / max(1, sum(pc.values()))\n",
    "    recall = common / max(1, sum(gc.values()))\n",
    "    f1 = 0.0 if (precision + recall) == 0 else 2 * precision * recall / (precision + recall)\n",
    "    return (precision, recall, f1)\n",
    "\n",
    "def keyword_f1(pred: str, gold: str) -> Tuple[float, float, float]:\n",
    "    pset = {t for t in sql_tokens(pred) if t in SQL_KEYWORDS}\n",
    "    gset = {t for t in sql_tokens(gold) if t in SQL_KEYWORDS}\n",
    "    if not pset and not gset:\n",
    "        return (1.0, 1.0, 1.0)\n",
    "    if not pset or not gset:\n",
    "        return (0.0, 0.0, 0.0)\n",
    "\n",
    "    common = len(pset & gset)\n",
    "    precision = common / len(pset)\n",
    "    recall = common / len(gset)\n",
    "    f1 = 0.0 if (precision + recall) == 0 else 2 * precision * recall / (precision + recall)\n",
    "    return (precision, recall, f1)\n",
    "\n",
    "def levenshtein(a: str, b: str) -> int:\n",
    "    \"\"\"Classic DP Levenshtein distance.\"\"\"\n",
    "    a = a or \"\"\n",
    "    b = b or \"\"\n",
    "    if a == b:\n",
    "        return 0\n",
    "    if len(a) == 0:\n",
    "        return len(b)\n",
    "    if len(b) == 0:\n",
    "        return len(a)\n",
    "\n",
    "    if len(a) > len(b):\n",
    "        a, b = b, a\n",
    "\n",
    "    prev = list(range(len(a) + 1))\n",
    "    for i, cb in enumerate(b, start=1):\n",
    "        cur = [i]\n",
    "        for j, ca in enumerate(a, start=1):\n",
    "            ins = cur[j-1] + 1\n",
    "            dele = prev[j] + 1\n",
    "            sub = prev[j-1] + (0 if ca == cb else 1)\n",
    "            cur.append(min(ins, dele, sub))\n",
    "        prev = cur\n",
    "    return prev[-1]\n",
    "\n",
    "def edit_similarity(pred: str, gold: str) -> float:\n",
    "    p = normalize_sql(pred)\n",
    "    g = normalize_sql(gold)\n",
    "    if not p and not g:\n",
    "        return 1.0\n",
    "    dist = levenshtein(p, g)\n",
    "    denom = max(1, max(len(p), len(g)))\n",
    "    return 1.0 - (dist / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6777d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_sql_api(api_url: str, user_query: str, timeout_s: int = 30) -> Dict[str, Any]:\n",
    "    payload = {\"user_query\": user_query}\n",
    "    r = requests.post(api_url, json=payload, timeout=timeout_s)\n",
    "    r.raise_for_status()\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f0de3",
   "metadata": {},
   "source": [
    "# LLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5171a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = \"\"\"You are a strict SQL evaluator.\n",
    "\n",
    "Task:\n",
    "Given (1) a natural-language question, (2) a GOLD SQL, and (3) a PREDICTED SQL,\n",
    "judge whether GOLD and PREDICTED are semantically equivalent for the same schema/data.\n",
    "\n",
    "Return JSON ONLY with keys:\n",
    "- \"equivalent\": true/false\n",
    "- \"score\": integer from 0 to 5  (5 = fully equivalent, 0 = totally wrong)\n",
    "- \"reason\": short explanation (<= 2 sentences)\n",
    "\n",
    "Be careful:\n",
    "- Different formatting is fine.\n",
    "- Different table aliases are fine.\n",
    "- If selected columns, filters, joins, grouping, or ordering change the meaning, it's NOT equivalent.\n",
    "\n",
    "NL_QUESTION:\n",
    "{question}\n",
    "\n",
    "GOLD_SQL:\n",
    "{gold}\n",
    "\n",
    "PREDICTED_SQL:\n",
    "{pred}\n",
    "\"\"\"\n",
    "\n",
    "def call_openai_judge(question: str, gold_sql: str, pred_sql: str) -> Dict[str, Any]:\n",
    "    prompt = JUDGE_PROMPT.format(question=question, gold=gold_sql, pred=pred_sql)\n",
    "\n",
    "    # Ask the model to return only JSON; we still parse defensively.\n",
    "    resp = client.responses.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        input=prompt,\n",
    "        # You can add reasoning controls for some models if desired (optional):\n",
    "        # reasoning={\"effort\": \"low\"},\n",
    "    )\n",
    "\n",
    "    text = (resp.output_text or \"\").strip()\n",
    "\n",
    "    # Extract JSON object robustly\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {\"equivalent\": None, \"score\": None, \"reason\": f\"Could not parse JSON. Raw: {text[:200]}\"}\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if \"equivalent\" not in obj or \"score\" not in obj or \"reason\" not in obj:\n",
    "            return {\"equivalent\": None, \"score\": None, \"reason\": f\"Invalid judge JSON: {obj}\"}\n",
    "        return obj\n",
    "    except Exception as e:\n",
    "        return {\"equivalent\": None, \"score\": None, \"reason\": f\"JSON parse error: {e}. Raw: {text[:200]}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ab5da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_csvs(dataset_dir: str) -> pd.DataFrame:\n",
    "    paths = sorted(glob.glob(os.path.join(dataset_dir, \"*.csv\")))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No CSV files found in '{dataset_dir}/'\")\n",
    "\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        df = pd.read_csv(p)\n",
    "        if not {\"user_query\", \"sql_query\"}.issubset(df.columns):\n",
    "            raise ValueError(f\"Missing required columns in {p}. Need: user_query, sql_query\")\n",
    "        df = df[[\"user_query\", \"sql_query\"]].copy()\n",
    "        df[\"source_file\"] = os.path.basename(p)\n",
    "        dfs.append(df)\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "    out[\"row_id\"] = range(len(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93b4d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "\n",
    "    df = load_all_csvs(DATASET_DIR)\n",
    "    results = []\n",
    "    judge_count = 0\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        user_q = str(row[\"user_query\"])\n",
    "        gold = str(row[\"sql_query\"])\n",
    "\n",
    "        t0 = time.time()\n",
    "        api_ok = True\n",
    "        api_err = None\n",
    "        pred = \"\"\n",
    "        relevant_tables = []\n",
    "        columns = []\n",
    "\n",
    "        try:\n",
    "            resp = call_sql_api(API_URL, user_q, timeout_s=API_TIMEOUT_S)\n",
    "            pred = resp.get(\"sql\", \"\") or \"\"\n",
    "            relevant_tables = resp.get(\"relevant_tables\", []) or []\n",
    "            columns = resp.get(\"columns\", []) or []\n",
    "        except Exception as e:\n",
    "            api_ok = False\n",
    "            api_err = str(e)\n",
    "\n",
    "        latency = time.time() - t0\n",
    "\n",
    "        # Metrics\n",
    "        exact = (pred == gold)\n",
    "        norm_exact = (normalize_sql(pred) == normalize_sql(gold))\n",
    "        p_tok, r_tok, f1_tok = token_f1(pred, gold)\n",
    "        p_kw, r_kw, f1_kw = keyword_f1(pred, gold)\n",
    "        ed_sim = edit_similarity(pred, gold)\n",
    "\n",
    "        # OpenAI judge (optional)\n",
    "        judge_equiv = None\n",
    "        judge_score = None\n",
    "        judge_reason = None\n",
    "\n",
    "        if USE_OPENAI_JUDGE and api_ok and pred.strip():\n",
    "            if (JUDGE_SAMPLE_LIMIT is None) or (judge_count < JUDGE_SAMPLE_LIMIT):\n",
    "                j = call_openai_judge(user_q, gold, pred)\n",
    "                judge_equiv = j.get(\"equivalent\")\n",
    "                judge_score = j.get(\"score\")\n",
    "                judge_reason = j.get(\"reason\")\n",
    "                judge_count += 1\n",
    "\n",
    "        results.append({\n",
    "            \"row_id\": int(row[\"row_id\"]),\n",
    "            \"source_file\": row[\"source_file\"],\n",
    "            \"user_query\": user_q,\n",
    "            \"gold_sql\": gold,\n",
    "            \"pred_sql\": pred,\n",
    "            \"api_ok\": api_ok,\n",
    "            \"api_error\": api_err,\n",
    "            \"latency_s\": latency,\n",
    "            \"exact_match\": exact,\n",
    "            \"normalized_exact_match\": norm_exact,\n",
    "            \"token_precision\": p_tok,\n",
    "            \"token_recall\": r_tok,\n",
    "            \"token_f1\": f1_tok,\n",
    "            \"keyword_precision\": p_kw,\n",
    "            \"keyword_recall\": r_kw,\n",
    "            \"keyword_f1\": f1_kw,\n",
    "            \"edit_similarity\": ed_sim,\n",
    "            \"judge_equivalent\": judge_equiv,\n",
    "            \"judge_score_0_5\": judge_score,\n",
    "            \"judge_reason\": judge_reason,\n",
    "            \"relevant_tables\": json.dumps(relevant_tables, ensure_ascii=False),\n",
    "            \"returned_columns\": json.dumps(columns, ensure_ascii=False),\n",
    "        })\n",
    "\n",
    "        if (i + 1) % 25 == 0:\n",
    "            print(f\"Processed {i+1}/{len(df)}\")\n",
    "\n",
    "    out = pd.DataFrame(results)\n",
    "\n",
    "    # Summary\n",
    "    def safe_mean(x):\n",
    "        xs = [v for v in x if v is not None and not (isinstance(v, float) and math.isnan(v))]\n",
    "        return statistics.mean(xs) if xs else None\n",
    "\n",
    "    summary = {\n",
    "        \"n\": len(out),\n",
    "        \"api_success_rate\": float(out[\"api_ok\"].mean()),\n",
    "        \"avg_latency_s\": float(out[\"latency_s\"].mean()),\n",
    "        \"exact_match_rate\": float(out[\"exact_match\"].mean()),\n",
    "        \"normalized_exact_match_rate\": float(out[\"normalized_exact_match\"].mean()),\n",
    "        \"avg_token_f1\": float(out[\"token_f1\"].mean()),\n",
    "        \"avg_keyword_f1\": float(out[\"keyword_f1\"].mean()),\n",
    "        \"avg_edit_similarity\": float(out[\"edit_similarity\"].mean()),\n",
    "        \"judge_used\": USE_OPENAI_JUDGE,\n",
    "        \"judge_rows\": int(out[\"judge_score_0_5\"].notna().sum()),\n",
    "        \"avg_judge_score_0_5\": safe_mean(out[\"judge_score_0_5\"].tolist()),\n",
    "        \"judge_equivalent_rate\": (\n",
    "            safe_mean([1.0 if v is True else 0.0 if v is False else None for v in out[\"judge_equivalent\"].tolist()])\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_path = os.path.join(REPORTS_DIR, f\"eval_results_{ts}.csv\")\n",
    "    json_path = os.path.join(REPORTS_DIR, f\"eval_summary_{ts}.json\")\n",
    "\n",
    "    out.to_csv(csv_path, index=False)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    print(f\"\\nSaved:\\n- {csv_path}\\n- {json_path}\")\n",
    "\n",
    "    return out, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc3b06c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY ===\n",
      "{\n",
      "  \"n\": 15,\n",
      "  \"api_success_rate\": 0.8,\n",
      "  \"avg_latency_s\": 14.163130458196004,\n",
      "  \"exact_match_rate\": 0.0,\n",
      "  \"normalized_exact_match_rate\": 0.06666666666666667,\n",
      "  \"avg_token_f1\": 0.56254590695965,\n",
      "  \"avg_keyword_f1\": 0.7205817711700065,\n",
      "  \"avg_edit_similarity\": 0.46726268151567546,\n",
      "  \"judge_used\": true,\n",
      "  \"judge_rows\": 12,\n",
      "  \"avg_judge_score_0_5\": 3.8333333333333335,\n",
      "  \"judge_equivalent_rate\": 0.5833333333333334\n",
      "}\n",
      "\n",
      "Saved:\n",
      "- ../results/eval_results_20251221_213255.csv\n",
      "- ../results/eval_summary_20251221_213255.json\n"
     ]
    }
   ],
   "source": [
    "out_df, summary = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb3ae0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   row_id source_file                                         user_query  \\\n",
       " 0       0    test.csv  Find the products supplied by more than 2 vend...   \n",
       " 1       1    test.csv  List the product categories along with the num...   \n",
       " 2       2    test.csv          Get the top 5 customers by total revenue.   \n",
       " 3       3    test.csv  Retrieve all products that have never been ord...   \n",
       " 4       4    test.csv  Get the total sales value for each month of ea...   \n",
       " 5       5     val.csv         Find the top-selling product in each year.   \n",
       " 6       6     val.csv  Retrieve the most expensive product in each ca...   \n",
       " 7       7     val.csv  Which products have the highest and lowest pro...   \n",
       " 8       8     val.csv              Show all sales orders placed in 2011.   \n",
       " 9       9     val.csv  Customers who purchased more than 100 differen...   \n",
       " \n",
       "                                             gold_sql  \\\n",
       " 0  SELECT ProductID, COUNT(*) AS VendorCount FROM...   \n",
       " 1  SELECT pc.ProductCategoryID, pc.Name AS Produc...   \n",
       " 2  SELECT c.CustomerID, SUM(soh.TotalDue) AS Reve...   \n",
       " 3  SELECT p.ProductID, p.Name FROM Product p LEFT...   \n",
       " 4  SELECT YEAR(OrderDate) AS Yr, MONTH(OrderDate)...   \n",
       " 5  SELECT Year, ProductID, Name, TotalQty FROM (S...   \n",
       " 6  SELECT pc.ProductCategoryID, pc.Name AS Catego...   \n",
       " 7  (SELECT 'Highest Margin' AS MarginType, p.Name...   \n",
       " 8  SELECT SalesOrderID, SalesOrderNumber, OrderDa...   \n",
       " 9  SELECT soh.CustomerID, COUNT(DISTINCT sod.Prod...   \n",
       " \n",
       "                                             pred_sql  api_ok  \\\n",
       " 0  SELECT pv.ProductID\\nFROM ProductVendor pv\\nGR...    True   \n",
       " 1  SELECT pc.Name, COUNT(ps.ProductSubcategoryID)...    True   \n",
       " 2  SELECT c.CustomerID, SUM(so.TotalDue) AS Total...    True   \n",
       " 3  SELECT p.ProductID, p.Name\\nFROM Product p\\nLE...    True   \n",
       " 4  SELECT \\n    YEAR(soh.OrderDate) AS Year, \\n  ...    True   \n",
       " 5                                                      False   \n",
       " 6                                                      False   \n",
       " 7  (\\n    SELECT p.ProductID, p.Name, (p.ListPric...    True   \n",
       " 8  SELECT soh.OrderDate, c.AccountNumber, b.Addre...    True   \n",
       " 9  SELECT c.CustomerID\\nFROM Customer c\\nJOIN Sal...    True   \n",
       " \n",
       "                                            api_error  latency_s  exact_match  \\\n",
       " 0                                               None   7.410394        False   \n",
       " 1                                               None   9.792678        False   \n",
       " 2                                               None   7.470263        False   \n",
       " 3                                               None   7.224064        False   \n",
       " 4                                               None   7.714193        False   \n",
       " 5  500 Server Error: Internal Server Error for ur...  32.094578        False   \n",
       " 6  500 Server Error: Internal Server Error for ur...  10.202949        False   \n",
       " 7                                               None  12.035398        False   \n",
       " 8                                               None  31.560008        False   \n",
       " 9                                               None  11.351725        False   \n",
       " \n",
       "    normalized_exact_match  ...  token_f1  keyword_precision  keyword_recall  \\\n",
       " 0                   False  ...  0.650000           0.833333        0.833333   \n",
       " 1                   False  ...  0.870588           1.000000        0.888889   \n",
       " 2                   False  ...  0.868421           1.000000        1.000000   \n",
       " 3                    True  ...  1.000000           1.000000        1.000000   \n",
       " 4                   False  ...  0.777778           1.000000        1.000000   \n",
       " 5                   False  ...  0.000000           0.000000        0.000000   \n",
       " 6                   False  ...  0.000000           0.000000        0.000000   \n",
       " 7                   False  ...  0.525952           0.818182        0.900000   \n",
       " 8                   False  ...  0.265306           0.500000        1.000000   \n",
       " 9                   False  ...  0.790123           1.000000        0.888889   \n",
       " \n",
       "    keyword_f1  edit_similarity  judge_equivalent  judge_score_0_5  \\\n",
       " 0    0.833333         0.427273              True              5.0   \n",
       " 1    0.941176         0.734982              True              5.0   \n",
       " 2    1.000000         0.754098              True              5.0   \n",
       " 3    1.000000         1.000000              True              5.0   \n",
       " 4    1.000000         0.814070              True              5.0   \n",
       " 5    0.000000         0.000000              None              NaN   \n",
       " 6    0.000000         0.000000              None              NaN   \n",
       " 7    0.857143         0.334294             False              2.0   \n",
       " 8    0.666667         0.234501             False              2.0   \n",
       " 9    0.941176         0.598214              True              5.0   \n",
       " \n",
       "                                         judge_reason  \\\n",
       " 0  Both SQL queries effectively count the number ...   \n",
       " 1  Both SQL queries accurately count the number o...   \n",
       " 2  Both SQL queries calculate the same value (tot...   \n",
       " 3  Both GOLD and PREDICTED SQL queries are identi...   \n",
       " 4  Both GOLD and PREDICTED SQLs correctly calcula...   \n",
       " 5                                               None   \n",
       " 6                                               None   \n",
       " 7  The GOLD_SQL calculates profit margins based o...   \n",
       " 8  The GOLD SQL queries only for SalesOrderID, Sa...   \n",
       " 9  Both queries count distinct products purchased...   \n",
       " \n",
       "                                relevant_tables  \\\n",
       " 0                            [\"ProductVendor\"]   \n",
       " 1    [\"ProductCategory\", \"ProductSubcategory\"]   \n",
       " 2             [\"Customer\", \"SalesOrderHeader\"]   \n",
       " 3              [\"Product\", \"SalesOrderDetail\"]   \n",
       " 4                         [\"SalesOrderHeader\"]   \n",
       " 5                                           []   \n",
       " 6                                           []   \n",
       " 7       [\"Product\", \"ProductListPriceHistory\"]   \n",
       " 8  [\"SalesOrderHeader\", \"Customer\", \"Address\"]   \n",
       " 9             [\"Customer\", \"SalesOrderDetail\"]   \n",
       " \n",
       "                                     returned_columns  \n",
       " 0                                      [\"ProductID\"]  \n",
       " 1                       [\"Name\", \"SubcategoryCount\"]  \n",
       " 2                     [\"CustomerID\", \"TotalRevenue\"]  \n",
       " 3                              [\"ProductID\", \"Name\"]  \n",
       " 4                    [\"Year\", \"Month\", \"TotalSales\"]  \n",
       " 5                                                 []  \n",
       " 6                                                 []  \n",
       " 7              [\"ProductID\", \"Name\", \"ProfitMargin\"]  \n",
       " 8  [\"OrderDate\", \"AccountNumber\", \"BillToAddress\"...  \n",
       " 9                                     [\"CustomerID\"]  \n",
       " \n",
       " [10 rows x 22 columns],\n",
       " {'n': 15,\n",
       "  'api_success_rate': 0.8,\n",
       "  'avg_latency_s': 14.163130458196004,\n",
       "  'exact_match_rate': 0.0,\n",
       "  'normalized_exact_match_rate': 0.06666666666666667,\n",
       "  'avg_token_f1': 0.56254590695965,\n",
       "  'avg_keyword_f1': 0.7205817711700065,\n",
       "  'avg_edit_similarity': 0.46726268151567546,\n",
       "  'judge_used': True,\n",
       "  'judge_rows': 12,\n",
       "  'avg_judge_score_0_5': 3.8333333333333335,\n",
       "  'judge_equivalent_rate': 0.5833333333333334})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.head(10), summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349832c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
